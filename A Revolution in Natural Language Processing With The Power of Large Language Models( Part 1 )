Every day, when I wake up and open my phone, the one thing I always see is the buzz of artificial intelligence all around us. Whether it’s notifications from Medium or the news feed on Google, the topic that pops up every time is artificial intelligence, and specifically, “ChatGPT”.So it is important for an Ai enthusiast to know about this tech and how it is revolutionising the future .Today, after reading this article, you will have a better understanding of how ChatGpt works and how to use it effectively .
Let’s start by understanding how this technology developed and its background because doing so will spark your interest in natural language processing and large language models, which might appear somewhat dry and complicated.

After World War II, the field of natural language processing (NLP ) was founded. People at the time realised the importance of translation between languages and intended to build a system that could carry out this kind of translation automatically. However, it was clear that the task was not as simple as people thought it was. Some researchers started to point out significant issues with the growth of NLP in 1958. Noam Chomsky was one of these researchers, and he found it disturbing because language models treated meaningless sentences that obeyed grammar rules to be insignificant in the same way as meaningless sentences that did not followed those rules. Further in 1957, Chomsky released his book “Syntactic Structures”. In it, he contradicted earlier theories about language, coming to the bold conclusion that a language’s structure of sentences needed to be changed in order for a computer to understand it. To achieve this, Chomsky developed an approach to grammar known as Phase-Structure Grammar, which carefully converted sentences from real language into a form that computers could understand. (The overall objective, was to develop a computer that could mimic the human brain in terms of thinking and communicating, or AI).

So this was a brief history to get your interest into Natural language Processing , moving forward we will be seeing how Large Language Models in short LLMs work and the how ChatGpt is able to give such good response’s. What i have understood is that these LLMs are nothing but just an extension of Machine Learning based NLP models. These models take large amount of data as input ( this data is in form of text ) and infer relationships between the words from the text! isn’t it mind blowing. So whenever you give an input to ChatGpt it starts predicting the next most suitable word that must be placed after your input prompt by using the concept of conditional probability that we all have learned in high school.For example i say : Jason is a good ______. Pause for a minute and think what could be placed next to good in the above fill in the blank. It’s obvious your mind generated terms like boy , man , person or human being , it didn’t think about something irrelevant like Jason is a good “Ball” or Jason is a good “Umbrella” and that’s exactly what’s happening with ChatGpt , it is searching for the best suitable word to be placed next to the prompt given by us. In our example our mind has a limited vocabulary and so we were able to generate limited number of words after the term good , but while training the neural network used in ChatGpt it was trained with a whopping dataset of around 300 billion words from books , articles , webtexts, Wikipedia and other pieces of writing on the internet. So as the size of dataset increases the capability of generating good responses also increases.

That is what LLM does, but for someone who enjoys learning more about a new technology’s inner workings in addition to utilising it, the question of how it does so is more crucial. The main focus is on word prediction, which can be done in either of the following ways: Next-Token Prediction and Masked-language Modelling. Let’s look at an example to better understand next-token prediction: Since we already stated that Jason is a good ______, let’s make the following scenarios: (i) Jason is a good __x__ , (ii) Jason is a good __y__ , (iii) Jason is a good__z__, (iv) Jason is a good __p__ and (v) Jason is a good __q__. So now the model has to predict what these token’s i.e x , y , z , p and q could be. This technique is quite simple to understand as the token with highest probability will be the answer to complete the sentence.

The second method is Masked-language Modelling. In this some words from a sentence are hidden or covered up, and the aim is to guess what those words should be based on the terms around them. This is an appreciated method for pre-training big neural language models in Natural Language Processing (NLP). This pre-training enhances the model’s ability to understand and produce natural language content by teaching it the contextual relationships between words.
Many NLP tasks, including sentiment analysis, question answering, and language translation, have proven to be successful when using masked language modelling.

So it is important to understand the working of this method , to simplify things lets again call Jason and take his help to understand. Lets say this time our sentence is : Jason ____ Physics here the 2nd word is hidden or masked between Jason and Physics and the model is find this hidden word based on the surrounding words i.e Jason and Physics or to be more precise the model needs to infer a relationship between Jason and Physics. The 2nd word can be : Jason Loves Physics , Jason hates Physics , Jason teaches Physics or even Jason studies Physics.

But there are certain limitations to these methods of predicting the next word and to overcome these limitations Transformers were developed , about which i will be discussing in the next article and there you will be discovering everything about ChatGpt , so to keep yourself updated stay tuned.
